

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Generating Images with Diffusion Models &mdash; GPI: GenAI-Powered Inference 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=320156fd" />

  
    <link rel="shortcut icon" href="_static/gpi.png"/>
    <link rel="canonical" href="https://gpi-pack.github.io/gen_diffusion.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/js/copybutton.js?v=a2f921dd"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Text-As-Treatment" href="tarnet.html" />
    <link rel="prev" title="Generating Texts without GPU" href="gen_nnsight.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="index.html" class="icon icon-home">
            GPI: GenAI-Powered Inference
              <img src="_static/logo_long.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-pypi">From PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpi.html">What’s GPI?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gpi.html#generative-ai-powered-inference">Generative-AI Powered Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="startgpu.html">How to use GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-s-gpu">What’s GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-if-you-do-not-have-gpu">What if you do not have GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#id1">Google Colaboratory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="report.html">Questions and Bug Reports</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Generation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gen_llama.html">Generating Texts with LLaMa3</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#how-to-use-llama3">How to use LLaMa3</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#creating-texts">Creating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#repeating-texts">Repeating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#system-prompt">System Prompt</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gen_llm.html">Generating Texts with Other LLMs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_llm.html#example-gemma2">Example: Gemma2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gen_nnsight.html">Generating Texts without GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_nnsight.html#nnsight-for-gpi">NNsight for GPI</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Generating Images with Diffusion Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-diffusion-model">What is diffusion model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-use-stable-diffusion">How to use Stable Diffusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#arguments">Arguments</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Operation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tarnet.html">Text-As-Treatment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#what-is-text-as-treatment">What is Text-As-Treatment?</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-estimate-treatment-effects">How to estimate treatment effects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tarnet.html#step-1-load-the-internal-representations">Step 1: Load the Internal Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="tarnet.html#step-2-estimate-the-treatment-effects">Step 2: Estimate the Treatment Effects</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-control-confounders">How to control confounders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tarnet.html#method-1-using-a-formula-with-a-dataframe">Method 1: Using a Formula with a DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="tarnet.html#method-2-using-a-design-matrix">Method 2: Using a Design Matrix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#visualizing-propensity-scores">Visualizing Propensity Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="text_as_confounder.html">Text-As-Confounder</a><ul>
<li class="toctree-l2"><a class="reference internal" href="text_as_confounder.html#what-is-text-as-confounder">What is Text-As-Confounder?</a></li>
<li class="toctree-l2"><a class="reference internal" href="text_as_confounder.html#how-to-estimate-treatment-effects">How to estimate treatment effects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="image_as_treatment.html">Image-As-Treatment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="image_as_treatment.html#how-to-estimate-treatment-effects">How to estimate treatment effects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="image_as_treatment.html#step-1-load-the-internal-representations">Step 1: Load the Internal Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="image_as_treatment.html#step-2-estimate-the-treatment-effects">Step 2: Estimate the Treatment Effects</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter.html">Hyperparameter Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#automated-hyperparameter-tuning">Automated Hyperparameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#list-of-hyperparameters">List of Hyperparameters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="custom_imp.html">Customizing Your Analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#tarnet">TarNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#propensity-score-model">Propensity Score Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#estimation-workflow">Estimation Workflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">When LLM is too big</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quantization.html#model-quantization">Model Quantization</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="f_dml_score.html">dml_score</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_dml_score.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_dml_score.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_dml_score.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_dml_score.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_k_ate.html">estimate_k_ate</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_k_ate.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_k_ate.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_k_ate.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_k_ate.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_psi_split.html">estimate_psi_split</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_psi_split.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_psi_split.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_psi_split.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_estimate_psi_split.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_extract_and_save_hiddens.html">extract_and_save_hiddens</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_extract_and_save_hiddens.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_extract_and_save_hiddens.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_extract_and_save_hiddens.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_extract_and_save_hiddens.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_generate_text.html">generate_text</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_generate_text.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_generate_text.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_generate_text.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_generate_text.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_get_instructions.html">get_instruction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_get_instructions.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_get_instructions.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_get_instructions.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_get_instructions.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_load_hiddens.html">load_hiddens</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_load_hiddens.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_load_hiddens.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_load_hiddens.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_load_hiddens.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_save_generated_texts.html">save_generated_texts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_save_generated_texts.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_save_generated_texts.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_save_generated_texts.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_save_generated_texts.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_SpectralNormClassifier.html">SpectralNormClassifier</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_SpectralNormClassifier.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_SpectralNormClassifier.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_SpectralNormClassifier.html#example-usage">Example Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_SpectralNormClassifier.html#methods">Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="f_SpectralNormClassifier.html#fit">fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="f_SpectralNormClassifier.html#predict-proba">predict_proba</a></li>
<li class="toctree-l3"><a class="reference internal" href="f_SpectralNormClassifier.html#predict">predict</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet_loss.html">TarNet_loss</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet_loss.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet_loss.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet_loss.html#returns">Returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet_loss.html#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet.html">TarNet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet.html#example-usage">Example Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNet.html#methods">Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="f_TarNet.html#fit">fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="f_TarNet.html#predict">predict</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetBase.html">TarNetBase</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetBase.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetBase.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetBase.html#example-usage">Example Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetBase.html#arguments">Arguments:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetHyperparameterTuner.html">TarNetHyperparameterTuner</a><ul>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetHyperparameterTuner.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetHyperparameterTuner.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="f_TarNetHyperparameterTuner.html#example-usage">Example Usage</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GPI: GenAI-Powered Inference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Generating Images with Diffusion Models</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="generating-images-with-diffusion-models">
<span id="generate-images"></span><h1>Generating Images with Diffusion Models<a class="headerlink" href="#generating-images-with-diffusion-models" title="Link to this heading"></a></h1>
<p>If you want to use GPI for image data, you need to generate images and extract the internal representations of deep generative models. This section describes how to generate images and extract the internal representations using <a class="reference external" href="https://huggingface.co/stabilityai">Stable Diffusion models</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For data generation, we recommend users to use GPUs. See <a class="reference internal" href="startgpu.html#gpu-usage-section"><span class="std std-ref">How to use GPU</span></a>.</p>
</div>
<section id="what-is-diffusion-model">
<h2>What is diffusion model?<a class="headerlink" href="#what-is-diffusion-model" title="Link to this heading"></a></h2>
<p>Diffusion models are a class of generative models that learn to generate data by modeling the diffusion process. They work by (1) gradually adding noise to the data and then (2) learning to reverse this process to generate new samples. This approach has been shown to produce high-quality samples in various domains, including images, videos, and audio. Below, we briefly give an overview of how one type of diffusion model, Stable Diffusion, works.</p>
<p>Stable diffusion model is a deep generative models developed by <a class="reference external" href="https://stability.ai/">stability.ai</a> and it is a type of diffusion model that uses a latent variable approach to generate high-quality images. It works by first encoding the input image into a lower-dimensional latent space, where the diffusion process is applied. The model then learns to reverse the diffusion process to generate new images from the latent space.</p>
<p>The stable diffusion model consists of the following key components:</p>
<ol class="arabic simple">
<li><p><strong>Autoencoder (VAE)</strong>: The variational autoencoder (VAE) is used to both encode the input image into a low-dimensional latent space and decode the latent representation back into an image. For example, we can convert an image of size (3, 512, 512) (3 color channels, 512 pixels height, 512 pixels width) into a latent representation of size (4, 64, 64) (4 channels, 64 pixels height, 64 pixels width).</p></li>
<li><p><strong>UNet</strong>: The U-Net architecture is used to predict the denoised image representation from the noisy latents.</p></li>
<li><p><strong>Text Encoder</strong>: The text encoder is used to enable users to condition the image generation process on text prompts. It encodes the text input into a latent representation that can be used to guide the image generation process.</p></li>
<li><p><strong>Scheduler</strong>: The scheduler is used to control the diffusion process, including the noise schedule and the number of diffusion steps. During the training phase, the scheduler adds noise to a sample to train a diffusion model. During the inference phase, the scheduler defines how to update a sample based on a pre-trained model’s output.</p></li>
</ol>
<p>In the context of GPI, we can use Stable Diffusion to generate images and extract the internal representations of the diffusion model. Since the internal representations within UNet is stochastic, we need to use the final output of the UNet (which is the input to the decoder) as the internal representation.</p>
</section>
<section id="how-to-use-stable-diffusion">
<h2>How to use Stable Diffusion<a class="headerlink" href="#how-to-use-stable-diffusion" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">extract_images</span></code> function provides a simple interface for generating images and extracting internal representations from Stable Diffusion models. This function is built on top of the Hugging Face <a class="reference external" href="https://huggingface.co/docs/diffusers/index">diffusers</a>  library and automates the process of image generation and internal representation extraction.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This functionality is added in version 0.1.1, and we currently only support the previous version of Stable Diffusion models (version 1.5 and version 2.1). We will support the latest version of these models in the future releases.</p>
</div>
<p>Here is an example of how to use this function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gpi_pack.diffusion</span><span class="w"> </span><span class="kn">import</span> <span class="n">extract_images</span>

<span class="c1"># Load toy images (from the website)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">io</span><span class="w"> </span><span class="kn">import</span> <span class="n">BytesIO</span>

<span class="c1"># Generate images using Stable Diffusion and save the hidden states</span>
<span class="n">extract_images</span><span class="p">(</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">img</span><span class="p">,</span> <span class="c1"># List of Input images or Images. If List is provided, the function will generate images based on each input images.</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="s2">&quot;no change&quot;</span><span class="p">,</span> <span class="c1"># Text prompts to condition the image generation process.</span>
    <span class="n">output_hidden_dir</span><span class="o">=</span> <span class="s2">&quot;/content/save&quot;</span><span class="p">,</span> <span class="c1"># Directory to save the internal representations of the diffusion model.</span>
    <span class="n">output_image_dir</span><span class="o">=</span> <span class="s2">&quot;/content/save&quot;</span><span class="p">,</span> <span class="c1"># Directory to save the generated images.</span>
    <span class="c1"># Optional parameters</span>
    <span class="n">save_name</span> <span class="o">=</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="c1"># Prefix for the saved image files</span>
    <span class="n">prefix_hidden</span> <span class="o">=</span> <span class="s2">&quot;hidden_&quot;</span><span class="p">,</span> <span class="c1"># Prefix for the saved internal representation files</span>
    <span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;runwayml/stable-diffusion-v1-5&quot;</span><span class="p">,</span> <span class="c1"># Model ID of the Stable Diffusion model to use. You can specify any model from Hugging Face&#39;s diffusers library.</span>
    <span class="n">strength</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># Strength of the image generation process (0.0 to 1.0). A higher value means more change from the input image. 0 means no change.</span>
    <span class="n">guidance_scale</span> <span class="o">=</span> <span class="mf">7.5</span><span class="p">,</span> <span class="c1"># Guidance scale for the text-to-image generation. Higher values lead to more adherence to the text prompt.</span>
    <span class="n">num_inference_steps</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="c1"># Number of diffusion steps to use for image generation</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="arguments">
<h2>Arguments<a class="headerlink" href="#arguments" title="Link to this heading"></a></h2>
<p>The function <code class="docutils literal notranslate"><span class="pre">extract_images</span></code> has the following arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">images</span></code>: input image(s) to transform. Accepts a single PIL.Image object, a single file path, or a list combining either type (required).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prompts</span></code>: text prompt(s) paired one‑to‑one with images. Accepts a single string or a list of strings (required).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_hidden_dir</span></code>: directory where extracted hidden‑state tensors (latents) will be saved (required).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_image_dir</span></code>: directory to save the generated (or reconstructed) images. If omitted, images are not written to disk (optional).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_name</span></code>: base filename stem used when saving images (e.g., gen_0.png). Default is “gen”.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefix_hidden</span></code>: prefix for hidden‑state files (e.g., hidden_0.pt). Default is “hidden”.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code>: Hugging Face identifier or local path of the Stable Diffusion model checkpoint to load. Default is “runwayml/stable-diffusion-v1-5”.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: compute device for inference (e.g., “cuda”, “cpu”). If None, chooses “cuda” when available, else “cpu” (optional).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cache_dir</span></code>: local directory for caching model weights. If None, uses Hugging Face’s default cache location (optional).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">strength</span></code>: how strongly the input image is altered (0 = no change, 1 = full generation). Float between 0 and 1. Default is 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_inference_steps</span></code>: number of diffusion denoising steps. Higher values yield higher quality but slower generation. Default is 50.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">guidance_scale</span></code>: classifier‑free guidance scale controlling prompt adherence. Typical range 5 – 15. Default is 7.5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">negative_prompt</span></code>: text that describes features to avoid in the generated images. If None, no negative prompt is used (optional).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">seed</span></code>: integer random seed for deterministic results. If None, the entire generation is nondeterministic (optional).</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gen_nnsight.html" class="btn btn-neutral float-left" title="Generating Texts without GPU" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tarnet.html" class="btn btn-neutral float-right" title="Text-As-Treatment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Kentaro Nakamura, Kosuke Imai.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>