<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Generating Texts with LLaMa3 &mdash; GPI: GenAI-Powered Inference 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=320156fd" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=01f34227"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/js/copybutton.js?v=a2f921dd"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Generating Texts with Other LLMs" href="gen_llm.html" />
    <link rel="prev" title="How to use GPU" href="startgpu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="index.html" class="icon icon-home">
            GPI: GenAI-Powered Inference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-pypi">From PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#requirements">Requirements</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpi.html">What’s GPI?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gpi.html#generative-ai-powered-inference">Generative-AI Powered Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpi.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="startgpu.html">How to use GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-s-gpu">What’s GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-if-you-do-not-have-gpu">What if you do not have GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#id1">Google Colaboratory</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Generation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Generating Texts with LLaMa3</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#how-to-use-llama3">How to use LLaMa3</a></li>
<li class="toctree-l2"><a class="reference internal" href="#creating-texts">Creating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#repeating-texts">Repeating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#system-prompts">System Prompts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gen_llm.html">Generating Texts with Other LLMs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_llm.html#example-gemma2">Example: Gemma2</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Operation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tarnet.html">Text-As-Treatment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#what-is-text-as-treatment">What is Text-As-Treatment?</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-estimate-treatment-effects">How to estimate treatment effects</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-control-confounders">How to control confounders</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#visualizing-propensity-scores">Visualizing Propensity Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter.html">Hyperparameter Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#automated-hyperparameter-tuning">Automated Hyperparameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#list-of-hyperparameters">List of Hyperparameters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="custom_imp.html">Customizing Your Analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#tarnet">TarNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#propensity-score-model">Propensity Score Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#estimation-workflow">Estimation Workflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">When LLM is too big</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quantization.html#model-quantization">Model Quantization</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">references</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="f_dml_score.html">dml_score</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_k_ate.html">estimate_k_ate</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_psi_split.html">estimate_psi_split</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_extract_and_save_hiddens.html">extract_and_save_hiddens</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_generate_text.html">generate_text</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_get_instructions.html">get_instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_load_hiddens.html">load_hiddens</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_save_generated_texts.html">save_generated_texts</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_SpectralNormClassifier.html">SpectralNormClassifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet_loss.html">TarNet_loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet.html">TarNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetBase.html">TarNetBase</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetHyperparameterTuner.html">TarNetHyperparameterTuner</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GPI: GenAI-Powered Inference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Generating Texts with LLaMa3</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="generating-texts-with-llama3">
<span id="generate-texts"></span><h1>Generating Texts with LLaMa3<a class="headerlink" href="#generating-texts-with-llama3" title="Link to this heading"></a></h1>
<p>For GPI, you need to generate texts and extract the internal representation of LLMs. This section describes how to generate texts using <a class="reference external" href="https://huggingface.co/meta-llama">LLaMa3</a>, which is one of the best open-source LLM.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For data generation, we recommend users to use GPUs. See <a class="reference internal" href="startgpu.html#gpu-usage-section"><span class="std std-ref">How to use GPU</span></a> for how to use GPUs.</p>
</div>
<section id="how-to-use-llama3">
<h2>How to use LLaMa3<a class="headerlink" href="#how-to-use-llama3" title="Link to this heading"></a></h2>
<p>LLaMa3 is a large language model developed by Meta AI. It is designed to be efficient and effective for a wide range of natural language processing tasks. LLaMa3 is available in different sizes and versions. You can choose the one that best fits your needs and computational resources. Here, I use <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B-Instruct</a>, which is the instruction-tuned version of LLaMa3 with 8 billion parameters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use Llama3, you need to have an access to the model (otherwise you will encounter the error “Cannot access gated repo for url”). You can request the access from <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">the model webpage</a>. Once you get access to the model, you also need to log in the huggingface and generate the access token. To generate access token, you need to click on “profile” button and select “Setting” and then click on “Access Token”. See <a class="reference external" href="https://huggingface.co/docs/hub/en/security-tokens">the huggingface website</a> for more details about access token.</p>
</div>
</section>
<section id="creating-texts">
<h2>Creating Texts<a class="headerlink" href="#creating-texts" title="Link to this heading"></a></h2>
<p><strong>gpi-pack</strong> provides a function <code class="docutils literal notranslate"><span class="pre">extract_and_save_hidden_states</span></code> to generate texts using LLaMa3 and extract its internal representation. To use this function, you first need to load LLM and the corresponding tokenizer. When you use LLaMa3, you must need to supply your huggingface access token to the <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>. Below is an example of how to load LLaMa3 and its tokenizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#loading required packages</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1">## Specify checkpoint (load LLaMa 3.1-8B)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;meta-llama/Meta-Llama-3.1-8B-Instruct&#39;</span> <span class="c1">#model checkpoint of LLaMa3.1-8B-Instruct</span>

<span class="c1">## Load tokenizer and pretrained model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">HUGGINGFACE</span> <span class="n">TOKEN</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Suppose that you have the following list of prompts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;Create a biography of an American politician named Nathaniel C. Gilchrist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Create a biography of an American politician named John Doe&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Create a biography of an American politician named Jane Smith&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Create a biography of an American politician named Mary Johnson&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Create a biography of an American politician named Robert Brown&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>You can generate texts and extract the internal representation of LLaMa3 using the following code. You need to specify the directory to save the hidden states and the file name to save the generated texts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gpi_pack.llm</span> <span class="kn">import</span> <span class="n">extract_and_save_hidden_states</span>

<span class="n">extract_and_save_hidden_states</span><span class="p">(</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">,</span>
    <span class="n">output_hidden_dir</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">HIDDEN</span> <span class="n">DIR</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1">#directory to save hidden states</span>
    <span class="n">save_name</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">SAVE</span> <span class="n">NAME</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1">#path and file name to save generated texts</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">task_type</span> <span class="o">=</span> <span class="s2">&quot;create&quot;</span> <span class="c1"># if you want to generate new texts, set task_type == &quot;create&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For GPI, it is required to use the deterministic decoding strategy. When you use <code class="docutils literal notranslate"><span class="pre">extract_and_save_hidden_states</span></code>, the function internally sets the decoding strategy to be deterministic (greedy decoding).</p>
</div>
</section>
<section id="repeating-texts">
<h2>Repeating Texts<a class="headerlink" href="#repeating-texts" title="Link to this heading"></a></h2>
<p>The function <code class="docutils literal notranslate"><span class="pre">extract_and_save_hidden_states</span></code> can also be used for the existing texts. To do so, you need to set <code class="docutils literal notranslate"><span class="pre">task_type</span> <span class="pre">=</span> <span class="pre">&quot;repeat&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gpi_pack.llm</span> <span class="kn">import</span> <span class="n">extract_and_save_hidden_states</span>

<span class="n">extract_and_save_hidden_states</span><span class="p">(</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">,</span> <span class="c1">#this text is the existing texts to be repeated</span>
    <span class="n">output_hidden_dir</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">HIDDEN</span> <span class="n">DIR</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1">#directory to save hidden states</span>
    <span class="n">save_name</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">SAVE</span> <span class="n">NAME</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1">#path and file name to save generated texts</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">task_type</span> <span class="o">=</span> <span class="s2">&quot;repeat&quot;</span> <span class="c1"># if you want to repeat existing texts, set task_type == &quot;repeat&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="arguments">
<h2>Arguments<a class="headerlink" href="#arguments" title="Link to this heading"></a></h2>
<p>The function <code class="docutils literal notranslate"><span class="pre">extract_and_save_hidden_states</span></code> has the following arguments.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prompts</span></code>: list of prompts to generate texts (<strong>required</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_hidden_dir</span></code>: directory to save the hidden states (<strong>required</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_name</span></code>: path and file name to save generated texts (<strong>required</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>: tokenizer of LLM (<strong>required</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: pretrained LLM (<strong>required</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task_type</span></code>: type of task. If you want to generate new texts, set <code class="docutils literal notranslate"><span class="pre">task_type</span> <span class="pre">=</span> <span class="pre">&quot;create&quot;</span></code>. If you want to repeat existing texts, set <code class="docutils literal notranslate"><span class="pre">task_type</span> <span class="pre">=</span> <span class="pre">&quot;repeat&quot;</span></code>. The default is “create”. You can also provide string that specifies the system-level inputs (explained below).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code>: maximum number of tokens to be generated. The default is 1000.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefix_hidden</span></code>: prefix of the hidden states files. The default is <code class="docutils literal notranslate"><span class="pre">hidden_</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer_config</span></code>: configuration of tokenizer (optional)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_config</span></code>: configuration of model (optional)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pooling</span></code>: pooling method to extract the internal representation. The default is “last”. You can also use “mean” or “max”.</p></li>
</ul>
</section>
<section id="system-prompts">
<h2>System Prompts<a class="headerlink" href="#system-prompts" title="Link to this heading"></a></h2>
<p>System prompt is a special type of prompt that is used to provide instructions or context to the LLM. The function <code class="docutils literal notranslate"><span class="pre">extract_and_save_hidden_states</span></code> instructs the task type (create or repeat) by using the system prompt. This function also allows you to specify your own system prompt by providing a string to <code class="docutils literal notranslate"><span class="pre">task_type</span></code>. Below is an example of how to use the system prompt.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gpi_pack.llm</span> <span class="kn">import</span> <span class="n">extract_and_save_hidden_states</span>

<span class="n">extract_and_save_hidden_states</span><span class="p">(</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">,</span>
    <span class="n">output_hidden_dir</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">HIDDEN</span> <span class="n">DIR</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">save_name</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">SAVE</span> <span class="n">NAME</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="c1">#supply the user-specified system prompt</span>
    <span class="n">task_type</span> <span class="o">=</span> <span class="s2">&quot;You are a text generator who always produces a biography of the instructed person&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="startgpu.html" class="btn btn-neutral float-left" title="How to use GPU" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gen_llm.html" class="btn btn-neutral float-right" title="Generating Texts with Other LLMs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Kentaro Nakamura, Kosuke Imai.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>