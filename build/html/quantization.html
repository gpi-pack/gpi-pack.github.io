<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>When LLM is too big &mdash; GPI: GenAI-Powered Inference 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=320156fd" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=01f34227"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/js/copybutton.js?v=a2f921dd"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="dml_score" href="f_dml_score.html" />
    <link rel="prev" title="Customizing Your Analysis" href="custom_imp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="index.html" class="icon icon-home">
            GPI: GenAI-Powered Inference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-pypi">From PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-source">From Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#requirements">Requirements</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpi.html">What’s GPI?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gpi.html#generative-ai-powered-inference">Generative-AI Powered Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpi.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="startgpu.html">How to use GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-s-gpu">What’s GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#what-if-you-do-not-have-gpu">What if you do not have GPU?</a></li>
<li class="toctree-l2"><a class="reference internal" href="startgpu.html#id1">Google Colaboratory</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Generation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gen_llama.html">Generating Texts with LLaMa3</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#how-to-use-llama3">How to use LLaMa3</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#creating-texts">Creating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#repeating-texts">Repeating Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#arguments">Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="gen_llama.html#system-prompts">System Prompts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gen_llm.html">Generating Texts with Other LLMs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gen_llm.html#example-gemma2">Example: Gemma2</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Operation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tarnet.html">Text-As-Treatment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#what-is-text-as-treatment">What is Text-As-Treatment?</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-estimate-treatment-effects">How to estimate treatment effects</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#how-to-control-confounders">How to control confounders</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#visualizing-propensity-scores">Visualizing Propensity Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="tarnet.html#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operations</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hyperparameter.html">Hyperparameter Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#automated-hyperparameter-tuning">Automated Hyperparameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="hyperparameter.html#list-of-hyperparameters">List of Hyperparameters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="custom_imp.html">Customizing Your Analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#tarnet">TarNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#propensity-score-model">Propensity Score Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_imp.html#estimation-workflow">Estimation Workflow</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">When LLM is too big</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-quantization">Model Quantization</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">references</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="f_dml_score.html">dml_score</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_k_ate.html">estimate_k_ate</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_estimate_psi_split.html">estimate_psi_split</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_extract_and_save_hiddens.html">extract_and_save_hiddens</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_generate_text.html">generate_text</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_get_instructions.html">get_instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_load_hiddens.html">load_hiddens</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_save_generated_texts.html">save_generated_texts</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_SpectralNormClassifier.html">SpectralNormClassifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet_loss.html">TarNet_loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNet.html">TarNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetBase.html">TarNetBase</a></li>
<li class="toctree-l1"><a class="reference internal" href="f_TarNetHyperparameterTuner.html">TarNetHyperparameterTuner</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GPI: GenAI-Powered Inference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">When LLM is too big</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="when-llm-is-too-big">
<h1>When LLM is too big<a class="headerlink" href="#when-llm-is-too-big" title="Link to this heading"></a></h1>
<p>It is often the case that the LLM is too big to be loaded into the memory. In these cases, you encounter the error message <code class="docutils literal notranslate"><span class="pre">CUDA</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">memory</span></code>. This error message means that your GPU does not have enough memory to load the LLM.</p>
<p>In that case, you can use the quantization technique to reduce the size of the model. The quantization technique reduces the precision of the model parameters, which can significantly reduce the size of the model. The quantized model can be loaded into the memory and used for generating texts.</p>
<section id="model-quantization">
<h2>Model Quantization<a class="headerlink" href="#model-quantization" title="Link to this heading"></a></h2>
<p>Here, we show how to use the quantization technique to reduce the size of the model (<a class="reference external" href="https://huggingface.co/meta-llama">LLaMa3</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The quantization technique is not supported for all LLMs. You need to check the documentation of the LLM to see if the quantization technique is supported.</p>
</div>
<p>To use the quantization technique, you need to install the <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes?bnb=4-bit">bitsandbytes</a> package. You can install it using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>bitsandbytes
</pre></div>
</div>
<p>Once you install the <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes?bnb=4-bit">bitsandbytes</a> package, you can use the quantization technique to reduce the size of the model. Below is an example of how to use the quantization technique to reduce the size of LLaMa3.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#loading required packages</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1">## Specify checkpoint (load LLaMa 3.1-8B)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;meta-llama/Meta-Llama-3.1-8B-Instruct&#39;</span> <span class="c1">#model checkpoint of LLaMa3.1-8B-Instruct</span>

<span class="c1">## Load tokenizer and pretrained model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">YOUR</span> <span class="n">HUGGINGFACE</span> <span class="n">TOKEN</span><span class="o">&gt;</span>
<span class="p">)</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># load the model in 4-bit</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span> <span class="c1"># quantization type</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="c1"># computation precision</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># use double quantization</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_imp.html" class="btn btn-neutral float-left" title="Customizing Your Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="f_dml_score.html" class="btn btn-neutral float-right" title="dml_score" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Kentaro Nakamura, Kosuke Imai.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>